{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n",
    "This notebook will give you an example of how to train a LightGBM model to estimate click-through rates on an e-commerce advertisement. We will train a LightGBM based model on the Criteo dataset.\n",
    "\n",
    "[LightGBM](https://github.com/Microsoft/LightGBM) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "* Fast training speed and high efficiency.\n",
    "* Low memory usage.\n",
    "* Great accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.7.13 (default, Mar 29 2022, 02:18:16) \n",
      "[GCC 7.5.0]\n",
      "LightGBM version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "from tempfile import TemporaryDirectory\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "import recommenders.models.lightgbm.lightgbm_utils as lgb_utils\n",
    "import recommenders.datasets.criteo as criteo\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"LightGBM version: {}\".format(lgb.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Setting\n",
    "Let's set the main related parameters for LightGBM now. Basically, the task is a binary classification (predicting click or no click), so the objective function is set to binary logloss, and 'AUC' metric, is used as a metric which is less effected by imbalance in the classes of the dataset.\n",
    "\n",
    "Generally, we can adjust the number of leaves (MAX_LEAF), the minimum number of data in each leaf (MIN_DATA), maximum number of trees (NUM_OF_TREES), the learning rate of trees (TREE_LEARNING_RATE) and EARLY_STOPPING_ROUNDS (to avoid overfitting) in the model to get better performance.\n",
    "\n",
    "Besides, we can also adjust some other listed parameters to optimize the results. [In this link](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst), a list of all the parameters is shown. Also, some advice on how to tune these parameters can be found [in this url](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-Tuning.rst). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MAX_LEAF = 64\n",
    "MIN_DATA = 20\n",
    "NUM_OF_TREES = 100\n",
    "TREE_LEARNING_RATE = 0.15\n",
    "EARLY_STOPPING_ROUNDS = 20\n",
    "METRIC = \"auc\"\n",
    "SIZE = \"sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_class': 1,\n",
    "    'objective': \"binary\",\n",
    "    'metric': METRIC,\n",
    "    'num_leaves': MAX_LEAF,\n",
    "    'min_data': MIN_DATA,\n",
    "    'boost_from_average': True,\n",
    "    #set it according to your cpu cores.\n",
    "    'num_threads': 24,\n",
    "    'feature_fraction': 0.8,\n",
    "    'learning_rate': TREE_LEARNING_RATE,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Here we use CSV format as the example data input. Our example data is a sample (about 100 thousand samples) from [Criteo dataset](https://www.kaggle.com/c/criteo-display-ad-challenge). The Criteo dataset is a well-known industry benchmarking dataset for developing CTR prediction models, and it's frequently adopted as evaluation dataset by research papers. The original dataset is too large for a lightweight demo, so we sample a small portion from it as a demo dataset.\n",
    "\n",
    "Specifically, there are 39 columns of features in Criteo, where 13 columns are numerical features (I1-I13) and the other 26 columns are categorical features (C1-C26)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baki/anaconda/envs/carve37/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (6,7,8,9,10,11,12,14,15,17,18,20,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>genres_0</th>\n",
       "      <th>genres_1</th>\n",
       "      <th>genres_2</th>\n",
       "      <th>genres_3</th>\n",
       "      <th>genres_4</th>\n",
       "      <th>categories_0</th>\n",
       "      <th>...</th>\n",
       "      <th>categories_2</th>\n",
       "      <th>color_0</th>\n",
       "      <th>color_1</th>\n",
       "      <th>color_2</th>\n",
       "      <th>tags_0</th>\n",
       "      <th>tags_1</th>\n",
       "      <th>tags_2</th>\n",
       "      <th>description_0</th>\n",
       "      <th>description_1</th>\n",
       "      <th>description_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5418</td>\n",
       "      <td>50997.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.198447e+09</td>\n",
       "      <td>Action</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>others_</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orange</td>\n",
       "      <td>Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>human face</td>\n",
       "      <td>text</td>\n",
       "      <td>poster</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44191</td>\n",
       "      <td>60716.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.437543e+09</td>\n",
       "      <td>Action</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>IMAX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people_show</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Red</td>\n",
       "      <td>Black</td>\n",
       "      <td>book</td>\n",
       "      <td>human face</td>\n",
       "      <td>album cover</td>\n",
       "      <td>candle</td>\n",
       "      <td>text</td>\n",
       "      <td>dark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60069</td>\n",
       "      <td>1691.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.534741e+09</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Children</td>\n",
       "      <td>Romance</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>text_mag</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>screenshot</td>\n",
       "      <td>cartoon</td>\n",
       "      <td>calendar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1376</td>\n",
       "      <td>152325.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.755375e+08</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people_</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Black</td>\n",
       "      <td>Pink</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>book</td>\n",
       "      <td>painting</td>\n",
       "      <td>human face</td>\n",
       "      <td>text</td>\n",
       "      <td>book</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>608</td>\n",
       "      <td>158007.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.320073e+08</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Crime</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text_sign</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stitch</td>\n",
       "      <td>text</td>\n",
       "      <td>embroidery</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId    userId  rating     timestamp   genres_0    genres_1   genres_2  \\\n",
       "0     5418   50997.0     5.0  1.198447e+09     Action     Mystery   Thriller   \n",
       "1    44191   60716.0     4.0  1.437543e+09     Action      Sci-Fi   Thriller   \n",
       "2    60069    1691.0     5.0  1.534741e+09  Adventure   Animation   Children   \n",
       "3     1376  152325.0     4.0  9.755375e+08  Adventure      Comedy     Sci-Fi   \n",
       "4      608  158007.0     4.0  8.320073e+08     Comedy       Crime      Drama   \n",
       "\n",
       "    genres_3 genres_4 categories_0  ... categories_2 color_0 color_1  color_2  \\\n",
       "0        NaN      NaN      others_  ...          NaN  Orange   Black      NaN   \n",
       "1       IMAX      NaN  people_show  ...          NaN   Brown     Red    Black   \n",
       "2    Romance   Sci-Fi     text_mag  ...          NaN   Black    Blue      NaN   \n",
       "3        NaN      NaN      people_  ...          NaN   Black    Pink   Yellow   \n",
       "4   Thriller      NaN    text_sign  ...          NaN   White     NaN      NaN   \n",
       "\n",
       "       tags_0       tags_1        tags_2 description_0 description_1  \\\n",
       "0  human face         text        poster          text           NaN   \n",
       "1        book   human face   album cover        candle          text   \n",
       "2        text   screenshot       cartoon      calendar           NaN   \n",
       "3        book     painting    human face          text          book   \n",
       "4      stitch         text    embroidery          text           NaN   \n",
       "\n",
       "  description_2  \n",
       "0           NaN  \n",
       "1          dark  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4           NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#posters = pd.read_csv(\"../01_prepare_data/ml-25m/poster-urls.csv\")\n",
    "#ratings = pd.read_csv(\"../01_prepare_data/ml-25m/ratings.csv\")\n",
    "#all_data = ratings.join(posters, on=\"movieId\", lsuffix=\"_ratings\", rsuffix=\"_posters\")\n",
    "all_data = pd.read_csv(\"../../carve/datasets/carve-movielens-prepared.csv\")\n",
    "all_data = all_data.drop(\"Unnamed: 0\", axis=1)\n",
    "all_data = all_data.sample(n=100000, random_state=0)\n",
    "all_data.reset_index(inplace=True)\n",
    "all_data = all_data.drop(\"index\", axis=1)\n",
    "\n",
    "display(all_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 21)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId              0\n",
      "userId               0\n",
      "rating               0\n",
      "timestamp            0\n",
      "genres_0             0\n",
      "genres_1         16163\n",
      "genres_2         45161\n",
      "genres_3         77107\n",
      "genres_4         93075\n",
      "categories_0      4942\n",
      "categories_1     50793\n",
      "categories_2     83622\n",
      "color_0              0\n",
      "color_1          34867\n",
      "color_2          83101\n",
      "tags_0              12\n",
      "tags_1             438\n",
      "tags_2            1303\n",
      "description_0      730\n",
      "description_1    30643\n",
      "description_2    58035\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(all_data.isnull().sum())\n",
    "#all_data = all_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we cut three sets (train_data (first 80%), valid_data (middle 10%) and test_data (last 10%)), cut from the original all data. <br>\n",
    "Notably, considering the Criteo is a kind of time-series streaming data, which is also very common in recommendation scenario, we split the data by its order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['movieId', 'userId', 'rating', 'timestamp', 'genres_0', 'genres_1',\n",
       "       'genres_2', 'genres_3', 'genres_4', 'categories_0', 'categories_1',\n",
       "       'categories_2', 'color_0', 'color_1', 'color_2', 'tags_0', 'tags_1',\n",
       "       'tags_2', 'description_0', 'description_1', 'description_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId                  6104\n",
      "userId                26203.0\n",
      "rating                    5.0\n",
      "timestamp        1112973160.0\n",
      "genres_0               Comedy\n",
      "genres_1                  NaN\n",
      "genres_2                  NaN\n",
      "genres_3                  NaN\n",
      "genres_4                  NaN\n",
      "categories_0         text_mag\n",
      "categories_1              NaN\n",
      "categories_2              NaN\n",
      "color_0                 Green\n",
      "color_1                 Black\n",
      "color_2                   NaN\n",
      "tags_0                   book\n",
      "tags_1                   text\n",
      "tags_2                 poster\n",
      "description_0            text\n",
      "description_1             NaN\n",
      "description_2             NaN\n",
      "Name: 70000, dtype: object\n",
      "(100000, 21)\n"
     ]
    }
   ],
   "source": [
    "# split data to 3 sets    \n",
    "length = len(all_data)\n",
    "train_data = all_data.loc[:0.8*length-1]\n",
    "valid_data = all_data.loc[0.8*length:0.9*length-1]\n",
    "test_data = all_data.loc[0.9*length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "### Ordinal Encoding\n",
    "Considering LightGBM could handle the low-frequency features and missing value by itself, for basic usage, we only encode the string-like categorical features by an ordinal encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: X: (80000, 20); Y: (80000,).\n",
      "Valid Data Shape: X: (10000, 20); Y: (10000,).\n",
      "Test Data Shape: X: (10000, 20); Y: (10000,).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>userId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>genres_0</th>\n",
       "      <th>genres_1</th>\n",
       "      <th>genres_2</th>\n",
       "      <th>genres_3</th>\n",
       "      <th>genres_4</th>\n",
       "      <th>categories_0</th>\n",
       "      <th>categories_1</th>\n",
       "      <th>categories_2</th>\n",
       "      <th>color_0</th>\n",
       "      <th>color_1</th>\n",
       "      <th>color_2</th>\n",
       "      <th>tags_0</th>\n",
       "      <th>tags_1</th>\n",
       "      <th>tags_2</th>\n",
       "      <th>description_0</th>\n",
       "      <th>description_1</th>\n",
       "      <th>description_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.198447e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.437543e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.534741e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9.755375e+08</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8.320073e+08</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId  userId     timestamp  genres_0  genres_1  genres_2  genres_3  \\\n",
       "0        1       1  1.198447e+09         1         1         1         1   \n",
       "1        2       2  1.437543e+09         1         2         1         2   \n",
       "2        3       3  1.534741e+09         2         3         2         3   \n",
       "3        4       4  9.755375e+08         2         4         3         1   \n",
       "4        5       5  8.320073e+08         3         5         4         4   \n",
       "\n",
       "   genres_4  categories_0  categories_1  categories_2  color_0  color_1  \\\n",
       "0         1             1             1             1        1        1   \n",
       "1         1             2             2             1        2        2   \n",
       "2         2             3             2             1        3        3   \n",
       "3         0             4             2             0        3        4   \n",
       "4         1             5             3             1        4        5   \n",
       "\n",
       "   color_2  tags_0  tags_1  tags_2  description_0  description_1  \\\n",
       "0        1       1       1       1              1              1   \n",
       "1        2       2       2       2              2              2   \n",
       "2        1       3       3       3              3              1   \n",
       "3        3       2       4       4              1              3   \n",
       "4        0       4       1       5              1              1   \n",
       "\n",
       "   description_2  \n",
       "0              1  \n",
       "1              2  \n",
       "2              1  \n",
       "3              0  \n",
       "4              1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_cols = [\"userId\", \"movieId\", \n",
    "    \"genres_0\", \"genres_1\", \"genres_2\", \"genres_3\", \"genres_4\", \"categories_0\", \"categories_1\", \"categories_2\", \"color_0\", \"color_1\", \"color_2\", \"tags_0\", \"tags_1\", \"tags_2\", \"description_0\", \"description_1\", \"description_2\"]\n",
    "label_col = \"rating\"\n",
    "\n",
    "ord_encoder = ce.ordinal.OrdinalEncoder(cols=cate_cols)\n",
    "\n",
    "def encode_csv(df, encoder, label_col, typ='fit'):\n",
    "    if typ == 'fit':\n",
    "        df = encoder.fit_transform(df)\n",
    "    else:\n",
    "        df = encoder.transform(df)\n",
    "    y = df[label_col].values\n",
    "    del df[label_col]\n",
    "    return df, y\n",
    "\n",
    "train_x, train_y = encode_csv(train_data, ord_encoder, label_col)\n",
    "valid_x, valid_y = encode_csv(valid_data, ord_encoder, label_col, 'transform')\n",
    "test_x, test_y = encode_csv(test_data, ord_encoder, label_col, 'transform')\n",
    "\n",
    "print('Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n'\n",
    "      .format(trn_x_shape=train_x.shape,\n",
    "              trn_y_shape=train_y.shape,\n",
    "              vld_x_shape=valid_x.shape,\n",
    "              vld_y_shape=valid_y.shape,\n",
    "              tst_x_shape=test_x.shape,\n",
    "              tst_y_shape=test_y.shape,))\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "When both hyper-parameters and data are ready, we can create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Contains only one class\n",
      "[LightGBM] [Info] Number of positive: 80000, number of negative: 0\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12837\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=1.000000 -> initscore=34.539576\n",
      "[LightGBM] [Info] Start training from score 34.539576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=1.000000 -> initscore=34.539576\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's auc: 1\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[7]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[8]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[9]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[10]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[11]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[12]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[13]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[14]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[15]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[16]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[17]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[18]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[19]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[20]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[21]\tvalid_0's auc: 1\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baki/anaconda/envs/carve37/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/baki/anaconda/envs/carve37/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/baki/anaconda/envs/carve37/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params, categorical_feature=cate_cols)\n",
    "lgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "lgb_test = lgb.Dataset(test_x, test_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "lgb_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=NUM_OF_TREES,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      valid_sets=lgb_valid,\n",
    "                      categorical_feature=cate_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what is the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_preds = lgb_model.predict(test_x)\\nauc = roc_auc_score(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\\nlogloss = log_loss(np.asarray(test_y.reshape(-1)), np.asarray(test_preds), eps=1e-12)\\nres_basic = {\"auc\": auc, \"logloss\": logloss}\\nprint(res_basic)\\nsb.glue(\"res_basic\", res_basic)\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test_preds = lgb_model.predict(test_x)\n",
    "auc = roc_auc_score(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "logloss = log_loss(np.asarray(test_y.reshape(-1)), np.asarray(test_preds), eps=1e-12)\n",
    "res_basic = {\"auc\": auc, \"logloss\": logloss}\n",
    "print(res_basic)\n",
    "sb.glue(\"res_basic\", res_basic)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "## Optimized Usage\n",
    "### Label-encoding and Binary-encoding\n",
    "Next, since LightGBM has a better capability in handling dense numerical features effectively, we try to convert all the categorical features in original data into numerical ones, by label-encoding [3] and binary-encoding [4]. Also due to the sequence property of Criteo, the label-encoding we adopted is executed one-by-one, which means we encode the samples in order, by the information of the previous samples before each sample (sequential label-encoding and sequential count-encoding). Besides, we also filter the low-frequency categorical features and fill the missing values by the mean of corresponding columns for the numerical features. (consulting `lgb_utils.NumEncoder`)\n",
    "\n",
    "Specifically, in `lgb_utils.NumEncoder`, the main steps are as follows.\n",
    "* Firstly, we convert the low-frequency categorical features to `\"LESS\"` and the missing categorical features to `\"UNK\"`. \n",
    "* Secondly, we convert the missing numerical features into the mean of corresponding columns. \n",
    "* Thirdly, the string-like categorical features are ordinal encoded like the example shown in basic usage. \n",
    "* And then, we target encode the categorical features in the samples order one-by-one. For each sample, we add the label and count information of its former samples into the data and produce new features. Formally, for $i=1,2,...,n$, we add $\\frac{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c) \\cdot y}{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c)}$ as a new label feature for current sample $x_i$, where $c$ is a category to encode in current sample, so $(i-1)$ is the number of former samples, and $I(\\cdot)$ is the indicator function that check the former samples contain $c$ (whether $x_j=c$) or not. At the meantime, we also add the count frequency of $c$, which is $\\frac{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c)}{i-1}$, as a new count feature. \n",
    "* Finally, based on the results of ordinal encoding, we add the binary encoding results as new columns into the data.\n",
    "\n",
    "Note that the statistics used in the above process only updates when fitting the training set, while maintaining static when transforming the testing set because the label of test data should be considered as unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 20:43:35,093 [INFO] Filtering and fillna features\n",
      "100%|██████████| 19/19 [00:01<00:00, 10.95it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2022-05-08 20:43:36,833 [INFO] Ordinal encoding cate features\n",
      "2022-05-08 20:43:37,420 [INFO] Target encoding cate features\n",
      "100%|██████████| 19/19 [00:01<00:00, 11.55it/s]\n",
      "2022-05-08 20:43:39,068 [INFO] Start manual binary encoding\n",
      "100%|██████████| 38/38 [00:02<00:00, 13.77it/s]\n",
      "100%|██████████| 19/19 [00:01<00:00, 12.10it/s]\n",
      "2022-05-08 20:43:43,505 [INFO] Filtering and fillna features\n",
      "100%|██████████| 19/19 [00:00<00:00, 416.94it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2022-05-08 20:43:43,554 [INFO] Ordinal encoding cate features\n",
      "2022-05-08 20:43:43,613 [INFO] Target encoding cate features\n",
      "100%|██████████| 19/19 [00:00<00:00, 90.48it/s]\n",
      "2022-05-08 20:43:43,825 [INFO] Start manual binary encoding\n",
      "100%|██████████| 38/38 [00:02<00:00, 14.26it/s]\n",
      "100%|██████████| 19/19 [00:01<00:00, 12.71it/s]\n",
      "2022-05-08 20:43:48,098 [INFO] Filtering and fillna features\n",
      "100%|██████████| 19/19 [00:00<00:00, 377.38it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2022-05-08 20:43:48,152 [INFO] Ordinal encoding cate features\n",
      "2022-05-08 20:43:48,212 [INFO] Target encoding cate features\n",
      "100%|██████████| 19/19 [00:00<00:00, 88.25it/s]\n",
      "2022-05-08 20:43:48,429 [INFO] Start manual binary encoding\n",
      "100%|██████████| 38/38 [00:02<00:00, 15.14it/s]\n",
      "100%|██████████| 19/19 [00:01<00:00, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: X: (80000, 156); Y: (80000, 1).\n",
      "Valid Data Shape: X: (10000, 156); Y: (10000, 1).\n",
      "Test Data Shape: X: (10000, 156); Y: (10000, 1).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_col = 'rating'\n",
    "nume_cols = []\n",
    "num_encoder = lgb_utils.NumEncoder(cate_cols, nume_cols, label_col)\n",
    "train_x, train_y = num_encoder.fit_transform(train_data)\n",
    "valid_x, valid_y = num_encoder.transform(valid_data)\n",
    "test_x, test_y = num_encoder.transform(test_data)\n",
    "del num_encoder\n",
    "print('Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n'\n",
    "      .format(trn_x_shape=train_x.shape,\n",
    "              trn_y_shape=train_y.shape,\n",
    "              vld_x_shape=valid_x.shape,\n",
    "              vld_y_shape=valid_y.shape,\n",
    "              tst_x_shape=test_x.shape,\n",
    "              tst_y_shape=test_y.shape,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baki/anaconda/envs/carve37/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Contains only one class\n",
      "[LightGBM] [Info] Number of positive: 80000, number of negative: 0\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9921\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=1.000000 -> initscore=34.539576\n",
      "[LightGBM] [Info] Start training from score 34.539576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=1.000000 -> initscore=34.539576\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's auc: 1\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[7]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[8]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[9]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[10]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[11]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[12]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[13]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[14]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[15]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[16]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[17]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[18]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[19]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[20]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[21]\tvalid_0's auc: 1\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 1\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params)\n",
    "lgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train)\n",
    "lgb_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=NUM_OF_TREES,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      valid_sets=lgb_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_preds = lgb_model.predict(test_x)\\nauc = roc_auc_score(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\\nlogloss = log_loss(np.asarray(test_y.reshape(-1)), np.asarray(test_preds), eps=1e-12)\\nres_optim = {\"auc\": auc, \"logloss\": logloss}\\nprint(res_optim)\\nsb.glue(\"res_optim\", res_optim)\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test_preds = lgb_model.predict(test_x)\n",
    "auc = roc_auc_score(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "logloss = log_loss(np.asarray(test_y.reshape(-1)), np.asarray(test_preds), eps=1e-12)\n",
    "res_optim = {\"auc\": auc, \"logloss\": logloss}\n",
    "print(res_optim)\n",
    "sb.glue(\"res_optim\", res_optim)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving and loading\n",
    "Now we finish the basic training and testing for LightGBM, next let's try to save and reload the model, and then evaluate it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nauc = roc_auc_score(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\\nlogloss = log_loss(np.asarray(test_y.reshape(-1)), np.asarray(test_preds), eps=1e-12)\\nprint({\"auc\": auc, \"logloss\": logloss})\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with TemporaryDirectory() as tmp:\n",
    "    save_file = os.path.join(tmp, r'finished.model')\n",
    "    lgb_model.save_model(save_file)\n",
    "    loaded_model = lgb.Booster(model_file=save_file)\n",
    "\n",
    "# eval the performance again\n",
    "test_preds = loaded_model.predict(test_x)\n",
    "\n",
    "\"\"\"\n",
    "auc = roc_auc_score(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "logloss = log_loss(np.asarray(test_y.reshape(-1)), np.asarray(test_preds), eps=1e-12)\n",
    "print({\"auc\": auc, \"logloss\": logloss})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading\n",
    "\n",
    "\\[1\\] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems. 3146–3154.<br>\n",
    "\\[2\\] The parameters of LightGBM: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst <br>\n",
    "\\[3\\] Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. 2018. CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363 (2018).<br>\n",
    "\\[4\\] Scikit-learn. 2018. categorical_encoding. https://github.com/scikit-learn-contrib/categorical-encoding<br>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "dc7f84238e46f2d072c968594e24b0589e9226683d9a55eb2ed173e38203d807"
  },
  "kernelspec": {
   "display_name": "Python (reco_pyspark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
