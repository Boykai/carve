{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation | Movielens 25M Dataset with Visual Enrichment\n",
    "\n",
    "## Movielens 25M Dataset\n",
    "This dataset leverages the [Movielens 25M Dataset](https://grouplens.org/datasets/movielens/25m/) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 25000095 ratings and 1093360 tag applications across 62423 movies. These data were created by 162541 users between January 09, 1995 and November 21, 2019. This dataset was generated on November 21, 2019.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The data are contained in the files genome-scores.csv, genome-tags.csv, links.csv, movies.csv, ratings.csv and tags.csv. More details about the contents and use of all these files follows.\n",
    "\n",
    "## Visual Enrichment\n",
    "The \"tmdbId\" column from the Movielens Dataset is utilized via the [The Movie Database (TMDb) API](https://www.themoviedb.org/documentation/api), in which the cooresponding movie poster url and image file are stored for later use in the enrichment process.\n",
    "\n",
    "Once movie posters for each movie are retrived, each movie poster image is sent to [Azure Computer Vision](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/overview) for analysis and metadata generation. The resulting features are then used to finally enrich the Movielens Dataset:\n",
    "* [Categories](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-categorizing-images)\n",
    "* [Color](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-detecting-color-schemes)\n",
    "* [Tags](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-tagging-images)\n",
    "* [Description](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-describing-images)\n",
    "* [Celebrities](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-detecting-domain-content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Global Variables\n",
    "SAMPLE_SIZE = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Movielens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baki/anaconda/envs/carve37/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (6,7,8,9,10,11,12,14,15,17,18,20,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>movieId</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>genres_0</th>\n",
       "      <th>genres_1</th>\n",
       "      <th>genres_2</th>\n",
       "      <th>genres_3</th>\n",
       "      <th>genres_4</th>\n",
       "      <th>...</th>\n",
       "      <th>categories_2</th>\n",
       "      <th>color_0</th>\n",
       "      <th>color_1</th>\n",
       "      <th>color_2</th>\n",
       "      <th>tags_0</th>\n",
       "      <th>tags_1</th>\n",
       "      <th>tags_2</th>\n",
       "      <th>description_0</th>\n",
       "      <th>description_1</th>\n",
       "      <th>description_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14781518</td>\n",
       "      <td>5418</td>\n",
       "      <td>50997.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.198447e+09</td>\n",
       "      <td>Action</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orange</td>\n",
       "      <td>Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>human face</td>\n",
       "      <td>text</td>\n",
       "      <td>poster</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17786929</td>\n",
       "      <td>44191</td>\n",
       "      <td>60716.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.437543e+09</td>\n",
       "      <td>Action</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>IMAX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Red</td>\n",
       "      <td>Black</td>\n",
       "      <td>book</td>\n",
       "      <td>human face</td>\n",
       "      <td>album cover</td>\n",
       "      <td>candle</td>\n",
       "      <td>text</td>\n",
       "      <td>dark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18917852</td>\n",
       "      <td>60069</td>\n",
       "      <td>1691.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.534741e+09</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Children</td>\n",
       "      <td>Romance</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>screenshot</td>\n",
       "      <td>cartoon</td>\n",
       "      <td>calendar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7110650</td>\n",
       "      <td>1376</td>\n",
       "      <td>152325.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.755375e+08</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Black</td>\n",
       "      <td>Pink</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>book</td>\n",
       "      <td>painting</td>\n",
       "      <td>human face</td>\n",
       "      <td>text</td>\n",
       "      <td>book</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3918965</td>\n",
       "      <td>608</td>\n",
       "      <td>158007.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.320073e+08</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Crime</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stitch</td>\n",
       "      <td>text</td>\n",
       "      <td>embroidery</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  movieId    userId  rating     timestamp   genres_0    genres_1  \\\n",
       "0  14781518     5418   50997.0     5.0  1.198447e+09     Action     Mystery   \n",
       "1  17786929    44191   60716.0     4.0  1.437543e+09     Action      Sci-Fi   \n",
       "2  18917852    60069    1691.0     5.0  1.534741e+09  Adventure   Animation   \n",
       "3   7110650     1376  152325.0     4.0  9.755375e+08  Adventure      Comedy   \n",
       "4   3918965      608  158007.0     4.0  8.320073e+08     Comedy       Crime   \n",
       "\n",
       "    genres_2   genres_3 genres_4  ... categories_2 color_0 color_1  color_2  \\\n",
       "0   Thriller        NaN      NaN  ...          NaN  Orange   Black      NaN   \n",
       "1   Thriller       IMAX      NaN  ...          NaN   Brown     Red    Black   \n",
       "2   Children    Romance   Sci-Fi  ...          NaN   Black    Blue      NaN   \n",
       "3     Sci-Fi        NaN      NaN  ...          NaN   Black    Pink   Yellow   \n",
       "4      Drama   Thriller      NaN  ...          NaN   White     NaN      NaN   \n",
       "\n",
       "       tags_0       tags_1        tags_2 description_0 description_1  \\\n",
       "0  human face         text        poster          text           NaN   \n",
       "1        book   human face   album cover        candle          text   \n",
       "2        text   screenshot       cartoon      calendar           NaN   \n",
       "3        book     painting    human face          text          book   \n",
       "4      stitch         text    embroidery          text           NaN   \n",
       "\n",
       "  description_2  \n",
       "0           NaN  \n",
       "1          dark  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4           NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../carve/datasets/carve-movielens-prepared.csv\")\n",
    "df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "df = df.sample(n=SAMPLE_SIZE, random_state=0)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SAR Single Node on MovieLens (Python, CPU)\n",
    "\n",
    "Simple Algorithm for Recommendation (SAR) is a fast and scalable algorithm for personalized recommendations based on user transaction history. It produces easily explainable and interpretable recommendations and handles \"cold item\" and \"semi-cold user\" scenarios. SAR is a kind of neighborhood based algorithm (as discussed in [Recommender Systems by Aggarwal](https://dl.acm.org/citation.cfm?id=2931100)) which is intended for ranking top items for each user. More details about SAR can be found in the [deep dive notebook](https://github.com/microsoft/recommenders/blob/main/examples/02_model_collaborative_filtering/sar_deep_dive.ipynb). \n",
    "\n",
    "SAR recommends items that are most ***similar*** to the ones that the user already has an existing ***affinity*** for. Two items are ***similar*** if the users that interacted with one item are also likely to have interacted with the other. A user has an ***affinity*** to an item if they have interacted with it in the past.\n",
    "\n",
    "#### Advantages of SAR:\n",
    "- High accuracy for an easy to train and deploy algorithm\n",
    "- Fast training, only requiring simple counting to construct matrices used at prediction time. \n",
    "- Fast scoring, only involving multiplication of the similarity matrix with an affinity vector\n",
    "\n",
    "#### Notes to use SAR properly:\n",
    "- Since it does not use item or user features, it can be at a disadvantage against algorithms that do.\n",
    "- It's memory-hungry, requiring the creation of an $mxm$ sparse square matrix (where $m$ is the number of items). This can also be a problem for many matrix factorization algorithms.\n",
    "- SAR favors an implicit rating scenario and it does not predict ratings.\n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate SAR in Python on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelSar(data, params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Import packages\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scrapbook as sb\n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "    from recommenders.utils.python_utils import binarize\n",
    "    from recommenders.utils.timer import Timer\n",
    "    from recommenders.datasets import movielens\n",
    "    from recommenders.datasets.python_splitters import python_stratified_split\n",
    "    from recommenders.evaluation.python_evaluation import (\n",
    "        map_at_k,\n",
    "        ndcg_at_k,\n",
    "        precision_at_k,\n",
    "        recall_at_k,\n",
    "        rmse,\n",
    "        mae,\n",
    "        logloss,\n",
    "        rsquared,\n",
    "        exp_var\n",
    "    )\n",
    "    from recommenders.models.sar import SAR\n",
    "    import sys\n",
    "\n",
    "    print(\"System version: {}\".format(sys.version))\n",
    "    print(\"Pandas version: {}\".format(pd.__version__))\n",
    "\n",
    "    #Global Variables\n",
    "    TOP_K = 10\n",
    "\n",
    "    #Start logging\n",
    "    #logging.basicConfig(level=logging.DEBUG, \n",
    "    #                format='%(asctime)s %(levelname)-8s %(message)s')\n",
    "    \n",
    "    #SAR Code\n",
    "    print(\"\\nStarting SAR...\")\n",
    "\n",
    "    #Convert the float precision to 32-bit in order to reduce memory consumption \n",
    "    data['rating'] = data['rating'].astype(np.float32)\n",
    "\n",
    "    #Split dataset\n",
    "    train, test = python_stratified_split(data, ratio=0.75, col_user='userId', col_item='movieId', seed=0)\n",
    "\n",
    "    #Print data summary\n",
    "    print(\"\"\"\n",
    "    Train:\n",
    "    Total Ratings: {train_total}\n",
    "    Unique Users: {train_users}\n",
    "    Unique Items: {train_items}\n",
    "\n",
    "    Test:\n",
    "    Total Ratings: {test_total}\n",
    "    Unique Users: {test_users}\n",
    "    Unique Items: {test_items}\n",
    "    \"\"\".format(\n",
    "        train_total=len(train),\n",
    "        train_users=len(train['userId'].unique()),\n",
    "        train_items=len(train['movieId'].unique()),\n",
    "        test_total=len(test),\n",
    "        test_users=len(test['userId'].unique()),\n",
    "        test_items=len(test['movieId'].unique()),\n",
    "    ))\n",
    "\n",
    "    #Instantiate the SAR algorithm and set the index\n",
    "    model = SAR(\n",
    "        col_user=\"userId\",\n",
    "        col_item=\"movieId\",\n",
    "        col_rating=\"rating\",\n",
    "        col_timestamp=\"timestamp\",\n",
    "        similarity_type=\"jaccard\", \n",
    "        time_decay_coefficient=30, \n",
    "        timedecay_formula=True,\n",
    "        normalize=True\n",
    "    )\n",
    "\n",
    "    #Train the SAR model on our training data\n",
    "    with Timer() as train_time:\n",
    "        model.fit(train)\n",
    "    print(\"Took {} seconds for training.\".format(train_time.interval))\n",
    "    \n",
    "    #Get the top-k recommendations for our testing data\n",
    "    with Timer() as test_time:\n",
    "        top_k = model.recommend_k_items(test, remove_seen=True)\n",
    "    print(\"Took {} seconds for prediction.\".format(test_time.interval))\n",
    "    \n",
    "    #Evaluate model\n",
    "    positivity_threshold = 2\n",
    "    test_bin = test.copy()\n",
    "    test_bin['rating'] = binarize(test_bin['rating'], positivity_threshold)\n",
    "\n",
    "    top_k_prob = top_k.copy()\n",
    "    top_k_prob['prediction'] = minmax_scale(\n",
    "    top_k_prob['prediction'].astype(float)\n",
    "    )\n",
    "\n",
    "    eval_map = map_at_k(test, top_k, col_user='userId', col_item='movieId', col_rating='rating', k=TOP_K)\n",
    "    eval_ndcg = ndcg_at_k(test, top_k, col_user='userId', col_item='movieId', col_rating='rating', k=TOP_K)\n",
    "    eval_precision = precision_at_k(test, top_k, col_user='userId', col_item='movieId', col_rating='rating', k=TOP_K)\n",
    "    eval_recall = recall_at_k(test, top_k, col_user='userId', col_item='movieId', col_rating='rating', k=TOP_K)\n",
    "    eval_rmse = rmse(test, top_k, col_user='userId', col_item='movieId', col_rating='rating')\n",
    "    eval_mae = mae(test, top_k, col_user='userId', col_item='movieId', col_rating='rating')\n",
    "    eval_rsquared = rsquared(test, top_k, col_user='userId', col_item='movieId', col_rating='rating')\n",
    "    eval_exp_var = exp_var(test, top_k, col_user='userId', col_item='movieId', col_rating='rating')\n",
    "    eval_logloss = logloss(test_bin, top_k_prob, col_user='userId', col_item='movieId', col_rating='rating')\n",
    "\n",
    "    evaluation_results = {\"Top K\": TOP_K,\n",
    "                        \"MAP\": eval_map,\n",
    "                        \"NDCG\": eval_ndcg,\n",
    "                        \"Precision\": eval_precision,\n",
    "                        \"Recall\": eval_recall,\n",
    "                        \"RMSE\": eval_rmse,\n",
    "                        \"MAE\": eval_mae,\n",
    "                        \"R2\": eval_rsquared,\n",
    "                        \"EXP-VAR\": eval_exp_var,\n",
    "                        \"Logloss\": eval_logloss}\n",
    "\n",
    "    print(\"Finished SAR...\\n\")\n",
    "    \n",
    "    return (model, evaluation_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n",
    "This notebook will give you an example of how to train a LightGBM model to estimate click-through rates on an e-commerce advertisement. We will train a LightGBM based model on the Criteo dataset.\n",
    "\n",
    "[LightGBM](https://github.com/Microsoft/LightGBM) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "* Fast training speed and high efficiency.\n",
    "* Low memory usage.\n",
    "* Great accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelLightgbm(data, params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Import packages\n",
    "    import sys\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import lightgbm as lgb\n",
    "    import papermill as pm\n",
    "    import scrapbook as sb\n",
    "    import pandas as pd\n",
    "    import category_encoders as ce\n",
    "    from tempfile import TemporaryDirectory\n",
    "    from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "    import recommenders.models.lightgbm.lightgbm_utils as lgb_utils\n",
    "    import recommenders.datasets.criteo as criteo\n",
    "\n",
    "    print(\"System version: {}\".format(sys.version))\n",
    "    print(\"LightGBM version: {}\".format(lgb.__version__))\n",
    "\n",
    "    #Global Variables\n",
    "    MAX_LEAF = 64\n",
    "    MIN_DATA = 20\n",
    "    NUM_OF_TREES = 100\n",
    "    TREE_LEARNING_RATE = 0.15\n",
    "    EARLY_STOPPING_ROUNDS = 20\n",
    "    METRIC = \"auc\"\n",
    "    SIZE = \"sample\"\n",
    "\n",
    "    #Start logging\n",
    "    #logging.basicConfig(level=logging.DEBUG, \n",
    "    #                format='%(asctime)s %(levelname)-8s %(message)s')\n",
    "\n",
    "    #Lightgbm Code\n",
    "    print(\"\\nStarting Lightgbm...\")\n",
    "\n",
    "    #Split dataset\n",
    "    length = len(data)\n",
    "    train_data = data.loc[:0.8*length-1]\n",
    "    valid_data = data.loc[0.8*length:0.9*length-1]\n",
    "    test_data = data.loc[0.9*length:]\n",
    "\n",
    "    #Encode the string-like categorical features by an ordinal encoder\n",
    "    cate_cols = [\"userId\", \n",
    "                \"movieId\", \n",
    "                \"genres_0\", \n",
    "                \"genres_1\", \n",
    "                \"genres_2\", \n",
    "                \"genres_3\", \n",
    "                \"genres_4\", \n",
    "                \"categories_0\", \n",
    "                \"categories_1\", \n",
    "                \"categories_2\", \n",
    "                \"color_0\", \n",
    "                \"color_1\", \n",
    "                \"color_2\", \n",
    "                \"tags_0\", \n",
    "                \"tags_1\", \n",
    "                \"tags_2\", \n",
    "                \"description_0\", \n",
    "                \"description_1\", \n",
    "                \"description_2\"]\n",
    "\n",
    "    label_col = \"rating\"\n",
    "\n",
    "    ord_encoder = ce.ordinal.OrdinalEncoder(cols=cate_cols)\n",
    "\n",
    "    def encode_csv(df, encoder, label_col, typ='fit'):\n",
    "        if typ == 'fit':\n",
    "            df = encoder.fit_transform(df)\n",
    "        else:\n",
    "            df = encoder.transform(df)\n",
    "        y = df[label_col].values\n",
    "        del df[label_col]\n",
    "        return df, y\n",
    "\n",
    "    train_x, train_y = encode_csv(train_data, ord_encoder, label_col)\n",
    "    valid_x, valid_y = encode_csv(valid_data, ord_encoder, label_col, 'transform')\n",
    "    test_x, test_y = encode_csv(test_data, ord_encoder, label_col, 'transform')\n",
    "\n",
    "    print('Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n'\n",
    "        .format(trn_x_shape=train_x.shape,\n",
    "                trn_y_shape=train_y.shape,\n",
    "                vld_x_shape=valid_x.shape,\n",
    "                vld_y_shape=valid_y.shape,\n",
    "                tst_x_shape=test_x.shape,\n",
    "                tst_y_shape=test_y.shape,))\n",
    "    \n",
    "    #Train the Lightgbm model on our training data\n",
    "    lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params, categorical_feature=cate_cols)\n",
    "    lgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "    lgb_test = lgb.Dataset(test_x, test_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "    model = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=NUM_OF_TREES,\n",
    "                    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                    valid_sets=lgb_valid,\n",
    "                    categorical_feature=cate_cols)\n",
    "\n",
    "    #TODO: Evaluation metrics\n",
    "    evaluation_results = dict()\n",
    "\n",
    "    print(\"Finished Lightgbm...\\n\")\n",
    "    \n",
    "    return (model, evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelLightgbmOptimized(data, params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Import packages\n",
    "    import sys\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import lightgbm as lgb\n",
    "    import papermill as pm\n",
    "    import scrapbook as sb\n",
    "    import pandas as pd\n",
    "    import category_encoders as ce\n",
    "    from tempfile import TemporaryDirectory\n",
    "    from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "    import recommenders.models.lightgbm.lightgbm_utils as lgb_utils\n",
    "    import recommenders.datasets.criteo as criteo\n",
    "\n",
    "    print(\"System version: {}\".format(sys.version))\n",
    "    print(\"LightGBM version: {}\".format(lgb.__version__))\n",
    "\n",
    "    #Global Variables\n",
    "\n",
    "    #Start logging\n",
    "    #logging.basicConfig(level=logging.DEBUG, \n",
    "    #                format='%(asctime)s %(levelname)-8s %(message)s')\n",
    "    \n",
    "    #Lightgbm Optimized Code\n",
    "    print(\"\\nStarting Lightgbm Optimized...\")\n",
    "\n",
    "    #Split dataset\n",
    "    length = len(data)\n",
    "    train_data = data.loc[:0.8*length-1]\n",
    "    valid_data = data.loc[0.8*length:0.9*length-1]\n",
    "    test_data = data.loc[0.9*length:]\n",
    "\n",
    "    #Encode the string-like categorical features by an ordinal encoder\n",
    "    cate_cols = [\"userId\", \n",
    "                \"movieId\", \n",
    "                \"genres_0\", \n",
    "                \"genres_1\", \n",
    "                \"genres_2\", \n",
    "                \"genres_3\", \n",
    "                \"genres_4\", \n",
    "                \"categories_0\", \n",
    "                \"categories_1\", \n",
    "                \"categories_2\", \n",
    "                \"color_0\", \n",
    "                \"color_1\", \n",
    "                \"color_2\", \n",
    "                \"tags_0\", \n",
    "                \"tags_1\", \n",
    "                \"tags_2\", \n",
    "                \"description_0\", \n",
    "                \"description_1\", \n",
    "                \"description_2\"]\n",
    "    label_col = 'rating'\n",
    "    nume_cols = []\n",
    "\n",
    "    #Convert all the categorical features in original data into numerical ones\n",
    "    num_encoder = lgb_utils.NumEncoder(cate_cols, nume_cols, label_col)\n",
    "    train_x, train_y = num_encoder.fit_transform(train_data)\n",
    "    valid_x, valid_y = num_encoder.transform(valid_data)\n",
    "    test_x, test_y = num_encoder.transform(test_data)\n",
    "    del num_encoder\n",
    "    print('Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n'\n",
    "        .format(trn_x_shape=train_x.shape,\n",
    "                trn_y_shape=train_y.shape,\n",
    "                vld_x_shape=valid_x.shape,\n",
    "                vld_y_shape=valid_y.shape,\n",
    "                tst_x_shape=test_x.shape,\n",
    "                tst_y_shape=test_y.shape,))\n",
    "\n",
    "    #Train the Lightgbm model on our training data\n",
    "    lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params)\n",
    "    lgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train)\n",
    "    model = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=NUM_OF_TREES,\n",
    "                    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                    valid_sets=lgb_valid)\n",
    "\n",
    "    #TODO: Evaluation metrics\n",
    "    evaluation_results = dict()\n",
    "\n",
    "    print(\"Finished Lightgbm Optimized...\\n\")\n",
    "    \n",
    "    return (model, evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "System version: 3.7.13 (default, Mar 29 2022, 02:18:16) \n",
      "[GCC 7.5.0]\n",
      "Pandas version: 1.3.5\n",
      "\n",
      "Starting SAR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 22:37:31,581 INFO     Collecting user affinity matrix\n",
      "2022-05-08 22:37:31,583 INFO     Calculating time-decayed affinities\n",
      "2022-05-08 22:37:31,598 INFO     Creating index columns\n",
      "2022-05-08 22:37:31,662 INFO     Calculating normalization factors\n",
      "2022-05-08 22:37:31,682 INFO     Building user affinity sparse matrix\n",
      "2022-05-08 22:37:31,684 INFO     Calculating item co-occurrence\n",
      "2022-05-08 22:37:31,695 INFO     Calculating item similarity\n",
      "2022-05-08 22:37:31,695 INFO     Using jaccard based similarity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Train:\n",
      "    Total Ratings: 87911\n",
      "    Unique Users: 55089\n",
      "    Unique Items: 8580\n",
      "\n",
      "    Test:\n",
      "    Total Ratings: 12089\n",
      "    Unique Users: 9774\n",
      "    Unique Items: 4419\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 22:37:33,096 INFO     Done training\n",
      "2022-05-08 22:37:33,101 INFO     Calculating recommendation scores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.5241332619989407 seconds for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 22:37:35,528 INFO     Removing seen items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2.8350970070023322 seconds for prediction.\n",
      "Finished SAR...\n",
      "\n",
      "System version: 3.7.13 (default, Mar 29 2022, 02:18:16) \n",
      "[GCC 7.5.0]\n",
      "LightGBM version: 3.3.2\n",
      "\n",
      "Starting Lightgbm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baki/anaconda/envs/carve37/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/baki/anaconda/envs/carve37/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/baki/anaconda/envs/carve37/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n",
      "2022-05-08 22:38:01,365 INFO     Filtering and fillna features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: X: (80000, 21); Y: (80000,).\n",
      "Valid Data Shape: X: (10000, 21); Y: (10000,).\n",
      "Test Data Shape: X: (10000, 21); Y: (10000,).\n",
      "\n",
      "[LightGBM] [Warning] Contains only one class\n",
      "[LightGBM] [Info] Number of positive: 80000, number of negative: 0\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13092\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=1.000000 -> initscore=34.539576\n",
      "[LightGBM] [Info] Start training from score 34.539576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=1.000000 -> initscore=34.539576\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's auc: 1\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[7]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[8]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[9]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[10]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[11]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[12]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[13]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[14]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[15]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[16]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[17]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[18]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[19]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[20]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[21]\tvalid_0's auc: 1\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 1\n",
      "Finished Lightgbm...\n",
      "\n",
      "System version: 3.7.13 (default, Mar 29 2022, 02:18:16) \n",
      "[GCC 7.5.0]\n",
      "LightGBM version: 3.3.2\n",
      "\n",
      "Starting Lightgbm Optimized...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00,  9.54it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2022-05-08 22:38:03,359 INFO     Ordinal encoding cate features\n",
      "2022-05-08 22:38:03,876 INFO     Target encoding cate features\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00, 10.30it/s]\n",
      "2022-05-08 22:38:05,723 INFO     Start manual binary encoding\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:03<00:00, 10.52it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:02<00:00,  9.44it/s]\n",
      "2022-05-08 22:38:11,509 INFO     Filtering and fillna features\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 418.94it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2022-05-08 22:38:11,558 INFO     Ordinal encoding cate features\n",
      "2022-05-08 22:38:11,615 INFO     Target encoding cate features\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 81.75it/s]\n",
      "2022-05-08 22:38:11,850 INFO     Start manual binary encoding\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:03<00:00, 11.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00, 10.51it/s]\n",
      "2022-05-08 22:38:17,217 INFO     Filtering and fillna features\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 367.41it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2022-05-08 22:38:17,273 INFO     Ordinal encoding cate features\n",
      "2022-05-08 22:38:17,336 INFO     Target encoding cate features\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 80.86it/s]\n",
      "2022-05-08 22:38:17,573 INFO     Start manual binary encoding\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:03<00:00, 10.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: X: (80000, 156); Y: (80000, 1).\n",
      "Valid Data Shape: X: (10000, 156); Y: (10000, 1).\n",
      "Test Data Shape: X: (10000, 156); Y: (10000, 1).\n",
      "\n",
      "[LightGBM] [Warning] Contains only one class\n",
      "[LightGBM] [Info] Number of positive: 80000, number of negative: 0\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9921\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=1.000000 -> initscore=34.539576\n",
      "[LightGBM] [Info] Start training from score 34.539576\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=1.000000 -> initscore=34.539576\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's auc: 1\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[7]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[8]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[9]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[10]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[11]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[12]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[13]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[14]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[15]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[16]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[17]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[18]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[19]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[20]\tvalid_0's auc: 1\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[21]\tvalid_0's auc: 1\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 1\n",
      "Finished Lightgbm Optimized...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run SAR\n",
    "results_sar = modelSar(df, None)\n",
    "\n",
    "#Run Lightgbm\n",
    "MAX_LEAF = 64\n",
    "MIN_DATA = 20\n",
    "NUM_OF_TREES = 100\n",
    "TREE_LEARNING_RATE = 0.15\n",
    "EARLY_STOPPING_ROUNDS = 20\n",
    "METRIC = \"auc\"\n",
    "SIZE = \"sample\"\n",
    "\n",
    "params_lightgbm = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_class': 1,\n",
    "    'objective': \"binary\",\n",
    "    'metric': METRIC,\n",
    "    'num_leaves': MAX_LEAF,\n",
    "    'min_data': MIN_DATA,\n",
    "    'boost_from_average': True,\n",
    "    #set it according to your cpu cores.\n",
    "    'num_threads': 24,\n",
    "    'feature_fraction': 0.8,\n",
    "    'learning_rate': TREE_LEARNING_RATE,\n",
    "}\n",
    "results_lightgbm = modelLightgbm(df, params_lightgbm)\n",
    "\n",
    "#Run Lightgbm Optimized\n",
    "results_lightgbm_optimized = modelLightgbmOptimized(df, params_lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SAR Results:\n",
      " {'Top K': 10, 'MAP': 0.0003698263290037384, 'NDCG': 0.0006328883367846422, 'Precision': 0.00016369961121342339, 'Recall': 0.0014835277266216492, 'RMSE': 3.1991763260010977, 'MAE': 3.081206230847868, 'R2': -13.31743533442921, 'EXP-VAR': -0.036446535106668954, 'Logloss': 5.403117521910442}\n",
      "\n",
      "Lightgbm Results:\n",
      " {}\n",
      "\n",
      "Lightgbm Optimized Results:\n",
      " {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "print(f\"\\n\\nSAR Results:\\n {results_sar[1]}\\n\")\n",
    "print(f\"Lightgbm Results:\\n {results_lightgbm[1]}\\n\")\n",
    "print(f\"Lightgbm Optimized Results:\\n {results_lightgbm_optimized[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top K</th>\n",
       "      <th>MAP</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "      <th>EXP-VAR</th>\n",
       "      <th>Logloss</th>\n",
       "      <th>Models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>3.199176</td>\n",
       "      <td>3.081206</td>\n",
       "      <td>-13.317435</td>\n",
       "      <td>-0.036447</td>\n",
       "      <td>5.403118</td>\n",
       "      <td>SAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lightgbm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lightgbm Optimized</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Top K      MAP      NDCG  Precision    Recall      RMSE       MAE  \\\n",
       "0   10.0  0.00037  0.000633   0.000164  0.001484  3.199176  3.081206   \n",
       "1    NaN      NaN       NaN        NaN       NaN       NaN       NaN   \n",
       "2    NaN      NaN       NaN        NaN       NaN       NaN       NaN   \n",
       "\n",
       "          R2   EXP-VAR   Logloss              Models  \n",
       "0 -13.317435 -0.036447  5.403118                 SAR  \n",
       "1        NaN       NaN       NaN            Lightgbm  \n",
       "2        NaN       NaN       NaN  Lightgbm Optimized  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame.from_dict([results_sar[1], results_lightgbm[1], results_lightgbm_optimized[1]])\n",
    "df_results[\"Models\"] = [\"SAR\",\n",
    "                        \"Lightgbm\",\n",
    "                        \"Lightgbm Optimized\"]\n",
    "df_results.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc7f84238e46f2d072c968594e24b0589e9226683d9a55eb2ed173e38203d807"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('carve37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
